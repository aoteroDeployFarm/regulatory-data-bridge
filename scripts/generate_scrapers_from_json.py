#!/usr/bin/env python3
"""
Generate HTML/PDF scraper scripts from a JSON configuration.

Usage:
  python generate_scrapers_from_json.py --config scrapers.json --outdir ./scrapers --overwrite

JSON formats supported:

1) List:
[
  {
    "name": "ca_cec",
    "target_url": "https://example.com/cec.html",
    "type": "html",
    "selector": "main, article, section, h1, h2, h3"
  },
  {
    "name": "epa_bulletin",
    "target_url": "https://example.com/bulletin.pdf",
    "type": "pdf"
  }
]

2) Object keyed by name:
{
  "ca_cec": {
    "target_url": "https://example.com/cec.html",
    "type": "html",
    "selector": "main, article"
  },
  "epa_bulletin": {
    "target_url": "https://example.com/bulletin.pdf",
    "type": "pdf"
  }
}

Fields:
- name        (optional but recommended): Used for filename if provided.
- target_url  (required): The source URL.
- type        (optional): "html" or "pdf". If omitted, inferred from URL extension.
- selector    (optional, HTML only): CSS selector to scope extraction.

Output:
- Writes `*_scraper.py` files to --outdir (default ./scrapers)
- Each scraper exposes: check_for_update(selector: str | None = None) -> dict
  with unified JSON schema:

{
  "url": "https://example.gov/file.pdf",
  "updated": true,
  "diffSummary": "PDF signature changed",
  "new_content": "…",
  "old_content": "…",
  "meta": {
    "content_type": "pdf|html",
    "selector_used": "… or null",
    "signature": "etag=…|lm=…|cl=… or sha256=…",
    "fetched_at": "2025-09-05T15:07:00Z"
  }
}

Dependencies (install in your runtime):
- HTML: beautifulsoup4 (and optionally lxml for speed)
- PDF: pypdf OR pdfminer.six
- HTTP: requests (fallback) or your shared httpx client (shared.http.get_client)
"""

from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from string import Template
from urllib.parse import urlparse


# ---------------------------
# Templates (HTML & PDF)
# ---------------------------

HTML_TEMPLATE = Template(r'''# Auto-generated by generate_scrapers_from_json.py (HTML)
from __future__ import annotations

from pathlib import Path
import hashlib
import json
from typing import Optional
from datetime import datetime, timezone

# HTML parsing
from bs4 import BeautifulSoup  # pip install beautifulsoup4

# --- Config ---
TARGET_URL = "$target_url"  # <-- Generated
DEFAULT_SELECTOR = "$default_selector"

CACHE_DIR = Path(__file__).parent / ".cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)
SIGNATURE_FILE = CACHE_DIR / "last_signature.json"
CONTENT_FILE = CACHE_DIR / "last_content.txt"

def _now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

# --- HTTP helpers ---
# Prefer shared httpx client; fall back to requests
try:
    from shared.http import get_client  # type: ignore

    def _head(url: str):
        c = get_client()
        r = c.head(url, timeout=15)
        r.raise_for_status()
        return r

    def _get_text(url: str) -> str:
        c = get_client()
        r = c.get(url, timeout=30)
        r.raise_for_status()
        return r.text

except Exception:  # pragma: no cover
    import requests  # type: ignore

    def _head(url: str):
        r = requests.head(url, allow_redirects=True, timeout=15)
        if r.status_code in (405, 403) or r.status_code >= 400:
            r = requests.get(url, stream=True, timeout=15)
        if r.status_code >= 400:
            r.raise_for_status()
        return r

    def _get_text(url: str) -> str:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        # decode with requests' detected encoding
        r.encoding = r.encoding or "utf-8"
        return r.text


def _build_signature_from_headers(headers) -> str:
    get_h = getattr(headers, "get", None)
    if callable(get_h):
        etag = headers.get("ETag", "") or headers.get("Etag", "")
        lm = headers.get("Last-Modified", "") or headers.get("Last-modified", "")
        cl = headers.get("Content-Length", "") or headers.get("Content-length", "")
    else:
        etag = lm = cl = ""
    if etag or lm or cl:
        return f"etag={etag}|lm={lm}|cl={cl}"
    return ""


def _hash_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8", "ignore")).hexdigest()


def _extract_html_content(html: str, selector: Optional[str]) -> str:
    soup = BeautifulSoup(html, "html.parser")
    sel = selector or DEFAULT_SELECTOR
    if sel:
        nodes = soup.select(sel)
        if nodes:
            return "\n\n".join(n.get_text(separator=" ", strip=True) for n in nodes)
    # fallback: whole page text
    return soup.get_text(separator=" ", strip=True)


def check_for_update(selector: str | None = None) -> dict:
    if not TARGET_URL or TARGET_URL == "{target_url}":
        return {
            "url": TARGET_URL,
            "updated": False,
            "diffSummary": "TARGET_URL is not set.",
            "new_content": None,
            "old_content": "",
            "meta": {
                "content_type": "html",
                "selector_used": selector or DEFAULT_SELECTOR,
                "signature": "",
                "fetched_at": _now_iso()
            },
            "error": "TARGET_URL placeholder not replaced"
        }

    new_signature = ""
    try:
        head_r = _head(TARGET_URL)
        new_signature = _build_signature_from_headers(getattr(head_r, "headers", {}))
    except Exception:
        # ignore header errors; we'll detect change by content hash
        new_signature = ""

    # Load last signature & content
    old_signature = ""
    old_content = ""
    if SIGNATURE_FILE.exists():
        try:
            cached = json.loads(SIGNATURE_FILE.read_text("utf-8"))
            old_signature = cached.get("signature", "")
        except json.JSONDecodeError:
            old_signature = ""
    if CONTENT_FILE.exists():
        try:
            old_content = CONTENT_FILE.read_text("utf-8")
        except Exception:
            old_content = ""

    # Fetch HTML body and compute body-hash
    try:
        html = _get_text(TARGET_URL)
    except Exception as e:
        return {
            "url": TARGET_URL,
            "updated": False,
            "diffSummary": f"Error fetching HTML: {e}",
            "new_content": None,
            "old_content": old_content,
            "meta": {
                "content_type": "html",
                "selector_used": selector or DEFAULT_SELECTOR,
                "signature": new_signature or "",
                "fetched_at": _now_iso()
            },
            "error": str(e)
        }

    body_hash = _hash_text(html)
    eff_signature = new_signature or f"sha256={body_hash}"
    is_updated = (eff_signature != old_signature)

    if is_updated:
        try:
            new_content = _extract_html_content(html, selector)
        except Exception as e:
            return {
                "url": TARGET_URL,
                "updated": False,
                "diffSummary": f"Error extracting HTML content: {e}",
                "new_content": None,
                "old_content": old_content,
                "meta": {
                    "content_type": "html",
                    "selector_used": selector or DEFAULT_SELECTOR,
                    "signature": eff_signature,
                    "fetched_at": _now_iso()
                },
                "error": str(e)
            }

        # Save caches
        try:
            SIGNATURE_FILE.write_text(json.dumps({"signature": eff_signature}), encoding="utf-8")
            CONTENT_FILE.write_text(new_content, encoding="utf-8")
        except Exception as e:
            return {
                "url": TARGET_URL,
                "updated": True,
                "diffSummary": f"Extracted new content, but failed to save cache: {e}",
                "new_content": new_content,
                "old_content": old_content,
                "meta": {
                    "content_type": "html",
                    "selector_used": selector or DEFAULT_SELECTOR,
                    "signature": eff_signature,
                    "fetched_at": _now_iso()
                },
                "error": str(e)
            }

        return {
            "url": TARGET_URL,
            "updated": True,
            "diffSummary": "HTML content changed",
            "new_content": new_content,
            "old_content": old_content,
            "meta": {
                "content_type": "html",
                "selector_used": selector or DEFAULT_SELECTOR,
                "signature": eff_signature,
                "fetched_at": _now_iso()
            }
        }

    # Not updated
    return {
        "url": TARGET_URL,
        "updated": False,
        "diffSummary": "No change",
        "new_content": None,
        "old_content": old_content,
        "meta": {
            "content_type": "html",
            "selector_used": selector or DEFAULT_SELECTOR,
            "signature": eff_signature,
            "fetched_at": _now_iso()
        }
    }


if __name__ == "__main__":
    print(json.dumps(check_for_update(), indent=2))
''')

PDF_TEMPLATE = Template(r'''# Auto-generated by generate_scrapers_from_json.py (PDF)
from __future__ import annotations

from pathlib import Path
import hashlib
import json
import io
from typing import Optional
from datetime import datetime, timezone

# PDF processing libraries (install one):
#   pip install pypdf
#   OR
#   pip install pdfminer.six
try:
    import pypdf  # type: ignore
except ImportError:  # pragma: no cover
    pypdf = None

try:
    from pdfminer.high_level import extract_text as pdfminer_extract_text  # type: ignore
except Exception:  # pragma: no cover
    pdfminer_extract_text = None

# --- Config ---
TARGET_URL = "$target_url"  # <-- Generated

CACHE_DIR = Path(__file__).parent / ".cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)
SIGNATURE_FILE = CACHE_DIR / "last_signature.json"
CONTENT_FILE = CACHE_DIR / "last_content.txt"

def _now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

# --- HTTP helpers ---
# Prefer shared httpx client; fall back to requests
try:
    from shared.http import get_client  # type: ignore

    def _head(url: str):
        c = get_client()
        r = c.head(url, timeout=15)
        r.raise_for_status()
        return r

    def _get_bytes(url: str) -> bytes:
        c = get_client()
        r = c.get(url, timeout=30)  # Increased timeout for full download
        r.raise_for_status()
        return r.content

except Exception:  # pragma: no cover
    import requests  # type: ignore

    def _head(url: str):
        r = requests.head(url, allow_redirects=True, timeout=15)
        if r.status_code in (405, 403) or r.status_code >= 400:
            r = requests.get(url, stream=True, timeout=15)
        if r.status_code >= 400:
            r.raise_for_status()
        return r

    def _get_bytes(url: str) -> bytes:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        return r.content


def _build_signature_from_headers(headers) -> str:
    get_h = getattr(headers, "get", None)
    if callable(get_h):
        etag = headers.get("ETag", "") or headers.get("Etag", "")
        lm = headers.get("Last-Modified", "") or headers.get("Last-modified", "")
        cl = headers.get("Content-Length", "") or headers.get("Content-length", "")
    else:
        etag = lm = cl = ""
    if etag or lm or cl:
        return f"etag={etag}|lm={lm}|cl={cl}"
    return ""


def _extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    """Extract text from PDF bytes using whichever library is available."""
    if pypdf:
        reader = pypdf.PdfReader(io.BytesIO(pdf_bytes))
        parts: list[str] = []
        for page in reader.pages:
            try:
                parts.append(page.extract_text() or "")
            except Exception:
                parts.append("")
        return "".join(parts)

    if pdfminer_extract_text:
        return pdfminer_extract_text(io.BytesIO(pdf_bytes)) or ""

    raise RuntimeError("No PDF text extraction library found. Install 'pypdf' or 'pdfminer.six'.")


def check_for_update(selector: str | None = None) -> dict:  # selector accepted for API parity; ignored
    if not TARGET_URL or TARGET_URL == "{target_url}":
        return {
            "url": TARGET_URL,
            "updated": False,
            "diffSummary": "TARGET_URL is not set.",
            "new_content": None,
            "old_content": "",
            "meta": {
                "content_type": "pdf",
                "selector_used": None,
                "signature": "",
                "fetched_at": _now_iso()
            },
            "error": "TARGET_URL placeholder not replaced"
        }

    new_signature = ""
    new_pdf_bytes: bytes = b""

    # Try signature from headers; else compute hash of full body
    try:
        head_r = _head(TARGET_URL)
        new_signature = _build_signature_from_headers(getattr(head_r, "headers", {}))
        if not new_signature:
            new_pdf_bytes = _get_bytes(TARGET_URL)
            new_signature = f"sha256={hashlib.sha256(new_pdf_bytes).hexdigest()}"
    except Exception as e:
        return {
            "url": TARGET_URL,
            "updated": False,
            "diffSummary": f"Error getting PDF signature: {e}",
            "new_content": None,
            "old_content": "",
            "meta": {
                "content_type": "pdf",
                "selector_used": None,
                "signature": "",
                "fetched_at": _now_iso()
            },
            "error": str(e)
        }

    # Load caches
    old_signature = ""
    old_content = ""
    if SIGNATURE_FILE.exists():
        try:
            cached = json.loads(SIGNATURE_FILE.read_text("utf-8"))
            old_signature = cached.get("signature", "")
        except json.JSONDecodeError:
            old_signature = ""
    if CONTENT_FILE.exists():
        try:
            old_content = CONTENT_FILE.read_text("utf-8")
        except Exception:
            old_content = ""

    is_updated = (new_signature != old_signature)

    if is_updated:
        if not new_pdf_bytes:
            try:
                new_pdf_bytes = _get_bytes(TARGET_URL)
            except Exception as e:
                return {
                    "url": TARGET_URL,
                    "updated": False,
                    "diffSummary": f"Error downloading PDF for content extraction: {e}",
                    "new_content": None,
                    "old_content": old_content,
                    "meta": {
                        "content_type": "pdf",
                        "selector_used": None,
                        "signature": new_signature,
                        "fetched_at": _now_iso()
                    },
                    "error": str(e)
                }

        try:
            new_content = _extract_text_from_pdf_bytes(new_pdf_bytes)
        except Exception as e:
            return {
                "url": TARGET_URL,
                "updated": False,
                "diffSummary": f"Error extracting text from PDF: {e}",
                "new_content": None,
                "old_content": old_content,
                "meta": {
                    "content_type": "pdf",
                    "selector_used": None,
                    "signature": new_signature,
                    "fetched_at": _now_iso()
                },
                "error": str(e)
            }

        # Save caches
        try:
            SIGNATURE_FILE.write_text(json.dumps({"signature": new_signature}), encoding="utf-8")
            CONTENT_FILE.write_text(new_content, encoding="utf-8")
        except Exception as e:
            return {
                "url": TARGET_URL,
                "updated": True,
                "diffSummary": f"Extracted new content, but failed to save cache: {e}",
                "new_content": new_content,
                "old_content": old_content,
                "meta": {
                    "content_type": "pdf",
                    "selector_used": None,
                    "signature": new_signature,
                    "fetched_at": _now_iso()
                },
                "error": str(e)
            }

        return {
            "url": TARGET_URL,
            "updated": True,
            "diffSummary": "PDF signature changed",
            "new_content": new_content,
            "old_content": old_content,
            "meta": {
                "content_type": "pdf",
                "selector_used": None,
                "signature": new_signature,
                "fetched_at": _now_iso()
            }
        }

    # Not updated
    return {
        "url": TARGET_URL,
        "updated": False,
        "diffSummary": "No change",
        "new_content": None,
        "old_content": old_content,
        "meta": {
            "content_type": "pdf",
            "selector_used": None,
            "signature": new_signature,
            "fetched_at": _now_iso()
        }
    }


if __name__ == "__main__":
    print(json.dumps(check_for_update(), indent=2))
''')


# ---------------------------
# Generator Script
# ---------------------------

def _infer_type_from_url(url: str) -> str:
    path = (urlparse(url).path or "").lower()
    if path.endswith(".pdf"):
        return "pdf"
    # crude heuristic: treat everything else as html
    return "html"


def _derive_filename(name: str | None, target_url: str, kind: str) -> str:
    if name:
        base = name.strip().lower().replace(" ", "_")
        suffix = "pdf_scraper.py" if kind == "pdf" else "html_scraper.py"
        return f"{base}_{suffix}"
    # derive from URL
    try:
        parsed = urlparse(target_url)
        segment = Path(parsed.path).name or "scraper"
        stem = Path(segment).stem or "scraper"
        suffix = "_pdf_scraper.py" if kind == "pdf" else "_html_scraper.py"
        return f"{stem}{suffix}"
    except Exception:
        return "scraper.py"


def _normalize_entries(raw):
    """
    Normalize config into a list of dicts:
      { name?, target_url, type?, selector? }
    """
    entries = []
    if isinstance(raw, list):
        for item in raw:
            if not isinstance(item, dict):
                continue
            entries.append({
                "name": item.get("name"),
                "target_url": item.get("target_url") or item.get("url"),
                "type": (item.get("type") or "").lower() or None,
                "selector": item.get("selector"),
                "default_selector": item.get("default_selector"),
                "filename": item.get("filename"),
            })
    elif isinstance(raw, dict):
        for k, v in raw.items():
            if not isinstance(v, dict):
                continue
            entries.append({
                "name": v.get("name") or k,
                "target_url": v.get("target_url") or v.get("url"),
                "type": (v.get("type") or "").lower() or None,
                "selector": v.get("selector"),
                "default_selector": v.get("default_selector"),
                "filename": v.get("filename"),
            })
    else:
        raise ValueError("Unsupported JSON format; must be list or object.")

    # filter invalid
    entries = [e for e in entries if e.get("target_url")]
    return entries


def render_scraper(kind: str, target_url: str, selector: str | None) -> str:
    if kind == "pdf":
        return PDF_TEMPLATE.substitute(target_url=target_url)
    # HTML
    default_selector = selector or "main, article, section, h1, h2, h3"
    return HTML_TEMPLATE.substitute(target_url=target_url, default_selector=default_selector)


def main():
    ap = argparse.ArgumentParser(description="Generate HTML/PDF scraper scripts from JSON.")
    ap.add_argument("--config", "-c", type=Path, required=True, help="Path to JSON config.")
    ap.add_argument("--outdir", "-o", type=Path, default=Path("./scrapers"), help="Output directory.")
    ap.add_argument("--overwrite", action="store_true", help="Overwrite existing files if present.")
    args = ap.parse_args()

    try:
        raw = json.loads(args.config.read_text(encoding="utf-8"))
    except Exception as e:
        print(f"Error reading config: {e}", file=sys.stderr)
        sys.exit(1)

    entries = _normalize_entries(raw)
    if not entries:
        print("No valid entries found in config (need at least one with target_url).", file=sys.stderr)
        sys.exit(2)

    args.outdir.mkdir(parents=True, exist_ok=True)

    written = 0
    skipped = 0
    for e in entries:
        target_url = e["target_url"]
        kind = e.get("type") or _infer_type_from_url(target_url)
        if kind not in ("html", "pdf"):
            kind = _infer_type_from_url(target_url)
        name = e.get("name")
        selector = e.get("selector")
        filename = e.get("filename") or _derive_filename(name, target_url, kind)
        outpath = args.outdir / filename

        if outpath.exists() and not args.overwrite:
            print(f"Skip (exists): {outpath}")
            skipped += 1
            continue

        code = render_scraper(kind, target_url, selector)
        try:
            outpath.write_text(code, encoding="utf-8")
            print(f"Wrote: {outpath}")
            written += 1
        except Exception as ex:
            print(f"Failed writing {outpath}: {ex}", file=sys.stderr)

    print(f"Done. Written: {written}, Skipped: {skipped}")


if __name__ == "__main__":
    main()
