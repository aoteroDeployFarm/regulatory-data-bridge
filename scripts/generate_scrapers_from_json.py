#!/usr/bin/env python3
"""
Generate many scrapers from a JSON list of state websites.

Basic:
  python scripts/generate_scrapers_from_json.py

Dry-run first:
  python scripts/generate_scrapers_from_json.py --dry-run

Subset (CA & CO, first 10 each):
  python scripts/generate_scrapers_from_json.py -s CA CO --max-per-state 10

Custom default selector (HTML only):
  python scripts/generate_scrapers_from_json.py --selector "article h2, .news a"

Selectors map (domain/path -> selector), e.g.:
  {
    "epa.gov/npdes-permits": "main h1, main h2, main a",
    "conservation.ca.gov": "h2, h3, .news a"
  }
  python scripts/generate_scrapers_from_json.py --selectors-file state-website-data/selectors.json
"""
from __future__ import annotations

import argparse, json, re, sys
from pathlib import Path
from typing import Dict, Iterable, List, Tuple
from urllib.parse import urlparse

REPO_ROOT = Path(__file__).resolve().parents[1]
DEFAULT_INPUT = REPO_ROOT / "state-website-data" / "state-website-data.json"

STATE_NAME_TO_CODE = {
    "alabama":"AL","alaska":"AK","arizona":"AZ","arkansas":"AR","california":"CA","colorado":"CO",
    "connecticut":"CT","delaware":"DE","florida":"FL","georgia":"GA","hawaii":"HI","idaho":"ID",
    "illinois":"IL","indiana":"IN","iowa":"IA","kansas":"KS","kentucky":"KY","louisiana":"LA",
    "maine":"ME","maryland":"MD","massachusetts":"MA","michigan":"MI","minnesota":"MN",
    "mississippi":"MS","missouri":"MO","montana":"MT","nebraska":"NE","nevada":"NV","new hampshire":"NH",
    "new jersey":"NJ","new mexico":"NM","new york":"NY","north carolina":"NC","north dakota":"ND",
    "ohio":"OH","oklahoma":"OK","oregon":"OR","pennsylvania":"PA","rhode island":"RI",
    "south carolina":"SC","south dakota":"SD","tennessee":"TN","texas":"TX","utah":"UT","vermont":"VT",
    "virginia":"VA","washington":"WA","west virginia":"WV","wisconsin":"WI","wyoming":"WY","district of columbia":"DC",
}

def info(msg: str): print(f"[info] {msg}")
def warn(msg: str): print(f"[warn] {msg}", file=sys.stderr)

def normalize_slug(netloc: str) -> str:
    slug = netloc.lower().replace("-", "_").replace(".", "_")
    slug = re.sub(r"[^a-z0-9_]+", "_", slug)
    return re.sub(r"_+", "_", slug).strip("_")

def ensure_pkg(path: Path):
    """Ensure package dirs have __init__.py (scrapers/, scrapers/state, scrapers/state/xx, target dir)."""
    parts = path.relative_to(REPO_ROOT).parts
    acc = REPO_ROOT
    for part in parts:
        acc = acc / part
        if acc.is_dir():
            init = acc / "__init__.py"
            if not init.exists():
                init.write_text("", encoding="utf-8")

HTML_TEMPLATE = """\
# Auto-generated by generate_scrapers_from_json.py
from __future__ import annotations
from pathlib import Path

TARGET_URL = "{target_url}"
CACHE_DIR = Path(__file__).parent / ".cache"

# Prefer shared HTTP helper; fall back to requests
try:
    from shared.http import fetch_text  # type: ignore
    def _fetch(url: str) -> str:
        return fetch_text(url)
except Exception:  # pragma: no cover
    import requests
    def _fetch(url: str) -> str:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        return r.text

try:
    from scrapers._base import check_updated  # type: ignore
except Exception as e:  # pragma: no cover
    raise RuntimeError("scrapers/_base.py is required. Please add the shared base helpers.") from e

def fetch_html() -> str:
    return _fetch(TARGET_URL)

def check_for_update() -> dict:
    return check_updated(
        fetch_html_fn=fetch_html,
        cache_dir=str(CACHE_DIR),
        selector={selector!r},
        url=TARGET_URL,
        diff_label={diff_label!r},
    )

if __name__ == "__main__":
    print(check_for_update())
"""

PDF_TEMPLATE = """\
# Auto-generated by generate_scrapers_from_json.py (PDF)
from __future__ import annotations
from pathlib import Path
import hashlib

TARGET_URL = "{target_url}"
CACHE_DIR = Path(__file__).parent / ".cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)
TOKEN_FILE = CACHE_DIR / "last_token.txt"

# Prefer shared httpx client; fall back to requests
try:
    from shared.http import get_client  # type: ignore
    def _head(url: str):
        c = get_client()
        r = c.head(url)
        r.raise_for_status()
        return r
    def _get_bytes(url: str) -> bytes:
        c = get_client()
        r = c.get(url)
        r.raise_for_status()
        return r.content
except Exception:  # pragma: no cover
    import requests
    def _head(url: str):
        r = requests.head(url, allow_redirects=True, timeout=15)
        if r.status_code >= 400:
            r.raise_for_status()
        return r
    def _get_bytes(url: str) -> bytes:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        return r.content

def _signature() -> str:
    try:
        r = _head(TARGET_URL)
        etag = r.headers.get("ETag", "")
        lm = r.headers.get("Last-Modified", "")
        cl = r.headers.get("Content-Length", "")
        if etag or lm or cl:
            return f"etag={etag}|lm={lm}|cl={cl}"
    except Exception:
        pass
    b = _get_bytes(TARGET_URL)
    h = hashlib.sha256(b).hexdigest()
    return f"sha256={h}|len={len(b)}"

def check_for_update() -> dict:
    new = _signature()
    old = TOKEN_FILE.read_text("utf-8").strip() if TOKEN_FILE.exists() else ""
    updated = (new != old)
    if updated:
        TOKEN_FILE.write_text(new, encoding="utf-8")
    return {{
        "url": TARGET_URL,
        "updated": updated,
        "diffSummary": "PDF signature changed" if updated else "No change",
    }}

if __name__ == "__main__":
    import json
    print(json.dumps(check_for_update(), indent=2))
"""

TEST_TEMPLATE = """\
# Auto-generated by generate_scrapers_from_json.py
from importlib import import_module

def test_{state_lc}_{slug}_imports_and_runs(monkeypatch):
    mod = import_module("{import_path}")
    # Fake network for HTML scrapers (PDF scrapers don't use fetch_html)
    if hasattr(mod, "fetch_html"):
        def fake_fetch():
            return "<html><h2>News</h2><a>Item A</a></html>"
        monkeypatch.setattr(mod, "fetch_html", fake_fetch)
    res = mod.check_for_update()
    assert isinstance(res, dict)
    assert "updated" in res and "url" in res
"""

def detect_mapping(obj) -> Dict[str, List[str]]:
    """Return {STATE_CODE: [urls,...]} for:
       {"Alabama":[...], ...} OR {"AL":[...], ...} OR {"states": {...}} OR list of {state,urls}."""
    # dict (maybe nested under "states")
    if isinstance(obj, dict):
        d = obj.get("states", obj)
        if isinstance(d, dict):
            out: Dict[str, List[str]] = {}
            for k, v in d.items():
                if not isinstance(v, (list, tuple)):
                    continue
                if re.fullmatch(r"[A-Za-z]{2}", k):
                    code = k.upper()
                else:
                    code = STATE_NAME_TO_CODE.get(k.strip().lower())
                    if not code:
                        warn(f"Unknown state name '{k}', skipping")
                        continue
                urls = [u for u in v if isinstance(u, str)]
                out[code] = urls
            if out:
                return out
    # list of {"state":..., "urls":[...]}
    if isinstance(obj, list):
        out: Dict[str, List[str]] = {}
        for item in obj:
            if not isinstance(item, dict):
                continue
            state_raw = item.get("state") or item.get("code") or item.get("name")
            urls = item.get("urls") or item.get("links") or []
            if not state_raw:
                continue
            sr = str(state_raw).strip()
            if re.fullmatch(r"[A-Za-z]{2}", sr):
                code = sr.upper()
            else:
                code = STATE_NAME_TO_CODE.get(sr.lower())
            if not code:
                warn(f"Unknown state '{state_raw}', skipping")
                continue
            if isinstance(urls, (list, tuple)):
                out.setdefault(code, [])
                out[code].extend([u for u in urls if isinstance(u, str)])
        if out:
            return out
    raise ValueError("Unrecognized JSON format for state website data")

def load_selectors_map(path: Path | None) -> List[tuple[str, str]]:
    """
    Return list of (needle, selector) sorted by needle length desc.
    Keys should be host or host/path substrings, e.g.:
      "epa.gov/npdes-permits": "main h1, main h2, main a"
      "conservation.ca.gov": "h2, h3, .news a"
    """
    if not path:
        return []
    data = json.loads(path.read_text(encoding="utf-8"))
    if not isinstance(data, dict):
        warn("selectors-file must be a JSON object; ignoring")
        return []
    items = []
    for k, v in data.items():
        if isinstance(k, str) and isinstance(v, str) and k.strip() and v.strip():
            items.append((k.strip().lower(), v.strip()))
    # Longer needles first (more specific)
    items.sort(key=lambda kv: len(kv[0]), reverse=True)
    return items

def pick_selector(url: str, default_selector: str, rules: List[tuple[str, str]]) -> str:
    key = url.lower()
    for needle, sel in rules:
        if needle in key:
            return sel
    return default_selector

def write_scraper(state_code: str, url: str, selector: str, *, pdf: bool, dry: bool=False, force: bool=False) -> Tuple[Path, Path]:
    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        raise ValueError(f"Invalid URL: {url}")
    slug = normalize_slug(parsed.netloc)
    pkg_dir = REPO_ROOT / "scrapers" / "state" / state_code.lower() / slug
    mod_path = f"scrapers.state.{state_code.lower()}.{slug}.check_updates"

    if dry:
        kind = "PDF" if pdf else "HTML"
        info(f"[DRY] {kind} -> {pkg_dir}/check_updates.py (TARGET_URL={url})")
        test_path = REPO_ROOT / "tests" / "test_scrapers" / f"test_{state_code.lower()}_{slug}_scraper.py"
        return (pkg_dir / "check_updates.py"), test_path

    pkg_dir.mkdir(parents=True, exist_ok=True)
    ensure_pkg(REPO_ROOT / "scrapers")
    ensure_pkg(REPO_ROOT / "scrapers" / "state")
    ensure_pkg(REPO_ROOT / "scrapers" / "state" / state_code.lower())
    ensure_pkg(pkg_dir)

    scraper_file = pkg_dir / "check_updates.py"
    if scraper_file.exists() and not force:
        warn(f"Exists, skipping (use --force): {scraper_file}")
    else:
        if pdf:
            code = PDF_TEMPLATE.format(target_url=url.strip())
        else:
            code = HTML_TEMPLATE.format(
                target_url=url.strip(),
                selector=selector.strip(),
                diff_label=f"{state_code} {parsed.netloc}",
            )
        scraper_file.write_text(code, encoding="utf-8")

    tests_dir = REPO_ROOT / "tests" / "test_scrapers"
    tests_dir.mkdir(parents=True, exist_ok=True)
    test_file = tests_dir / f"test_{state_code.lower()}_{slug}_scraper.py"
    if test_file.exists() and not force:
        warn(f"Test exists, skipping: {test_file}")
    else:
        test_code = TEST_TEMPLATE.format(
            state_lc=state_code.lower(),
            slug=slug,
            import_path=mod_path,
        )
        test_file.write_text(test_code, encoding="utf-8")

    return scraper_file, test_file

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", default=str(DEFAULT_INPUT), help="Path to JSON file")
    ap.add_argument("-s", "--states", nargs="*", help="Only these states (codes or names)")
    ap.add_argument("--exclude", nargs="*", help="Skip these states")
    ap.add_argument("--max-per-state", type=int, default=None, help="Limit number per state")
    ap.add_argument("--selector", default="h1, h2, h3, a", help="Default CSS selector (HTML)")
    ap.add_argument("--selectors-file", type=str, default=None, help="JSON file mapping domain/path -> selector")
    ap.add_argument("--force", action="store_true", help="Overwrite existing files")
    ap.add_argument("--dry-run", action="store_true", help="Plan only; donâ€™t write files")
    args = ap.parse_args()

    input_path = Path(args.input)
    if not input_path.exists():
        sys.exit(f"Input not found: {input_path}")

    # filters
    include_states: Iterable[str] | None = None
    if args.states:
        tmp = []
        for x in args.states:
            x = x.strip()
            if re.fullmatch(r"[A-Za-z]{2}", x):
                tmp.append(x.upper())
            else:
                code = STATE_NAME_TO_CODE.get(x.lower())
                if code: tmp.append(code)
        include_states = set(tmp)

    exclude_states: set[str] = set()
    if args.exclude:
        for x in args.exclude:
            if re.fullmatch(r"[A-Za-z]{2}", x):
                exclude_states.add(x.upper())
            else:
                code = STATE_NAME_TO_CODE.get(x.lower())
                if code: exclude_states.add(code)

    data = json.loads(input_path.read_text(encoding="utf-8"))
    mapping = detect_mapping(data)  # { "CA": [urls], ... }

    rules = load_selectors_map(Path(args.selectors_file)) if args.selectors_file else []

    total = 0
    for state_code, urls in sorted(mapping.items()):
        if include_states and state_code not in include_states:
            continue
        if state_code in exclude_states:
            continue
        info(f"State {state_code}: {len(urls)} url(s)")

        seen: set[str] = set()
        count = 0
        for u in urls:
            parsed = urlparse(u)
            if not parsed.scheme or not parsed.netloc:
                continue
            first_seg = (parsed.path.split("/")[1] if parsed.path else "")
            key = f"{parsed.netloc}|{first_seg}"
            if key in seen:
                continue
            seen.add(key)

            is_pdf = parsed.path.lower().endswith(".pdf")
            sel = pick_selector(u, args.selector, rules)
            try:
                write_scraper(state_code, u, selector=sel, pdf=is_pdf, dry=args.dry_run, force=args.force)
                total += 1
                count += 1
            except Exception as e:
                warn(f"Skipping {u}: {e}")

            if args.max_per_state and count >= args.max_per_state:
                break

    info(f"Done. {'(dry run) ' if args.dry_run else ''}Total new/updated scrapers: {total}")
    if not args.dry_run:
        info("Next: pytest -q && uvicorn services.web_api.app:create_app --factory --reload && GET /scrapers")

if __name__ == "__main__":
    sys.exit(main())
