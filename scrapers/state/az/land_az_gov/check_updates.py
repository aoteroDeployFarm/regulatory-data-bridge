# Auto-generated by generate_scrapers_from_json.py
from __future__ import annotations
from pathlib import Path

TARGET_URL = "https://land.az.gov/applications-permits"
CACHE_DIR = Path(__file__).parent / ".cache"

# Prefer shared HTTP helper; fall back to requests
try:
    from shared.http import fetch_text  # type: ignore
    def _fetch(url: str) -> str:
        return fetch_text(url)
except Exception:  # pragma: no cover
    import requests
    def _fetch(url: str) -> str:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        return r.text

try:
    from scrapers._base import check_updated  # type: ignore
except Exception as e:  # pragma: no cover
    raise RuntimeError("scrapers/_base.py is required. Please add the shared base helpers.") from e

def fetch_html() -> str:
    return _fetch(TARGET_URL)

def check_for_update() -> dict:
    return check_updated(
        fetch_html_fn=fetch_html,
        cache_dir=str(CACHE_DIR),
        selector='h1, h2, h3, a',
        url=TARGET_URL,
        diff_label='AZ land.az.gov',
    )

if __name__ == "__main__":
    print(check_for_update())
